{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import folium\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "mpl.rcParams['figure.figsize'] = (8,6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "# Setup the module path that contains the framework code\n",
    "from pathlib import Path\n",
    "module_path = str(Path.cwd())\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import the libraries\n",
    "from modules.DataOperations import *\n",
    "from modules.DataProcessing import *\n",
    "from modules.MachineLearner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inc_data_path = 'data/ecoregion/inc_count.csv'\n",
    "pop_data_path = 'data/ecoregion/pop.csv'\n",
    "env_data_path = 'data/ecoregion/env_data.pkl'\n",
    "ecoregion_data_path = 'data/ecoregion/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_performance_improvements(improve_df):\n",
    "\n",
    "    improve_df = improve_df*100\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    model_labels = ['Linear', 'LSTM', 'GRU']\n",
    "    \n",
    "    width = 0.1\n",
    "    plt.figure(num=None, figsize=(14,10))\n",
    "    \n",
    "    \n",
    "    df_indices = improve_df.index.values\n",
    "    xOffset = np.linspace(-0.35,0.35,8)\n",
    "    markerFmts = ['o', '<', 's', '*', 'p', 'x', 'D', '^']\n",
    "    labelArray = ['Fixed, $M^f$ = 5', 'Fixed, $M^f$ = 10', 'Fixed, $M^f$ = 20', 'Fixed, $M^f$ = 40', 'Detected, $\\Delta_{MIN}$ = 5', 'Detected, $\\Delta_{MIN}$ = 10', 'Detected, $\\Delta_{MIN}$ = 20', 'Detected, $\\Delta_{MIN}$ = 30']\n",
    "    \n",
    "    for i in range(len(df_indices)):\n",
    "        plt.plot(x+xOffset[i], improve_df.loc[df_indices[i]].values, markerFmts[i], markersize = 10, label= labelArray[i])\n",
    "    \n",
    "    plt.xticks(ticks=x, labels=model_labels)\n",
    "    plt.ylim([-10,40])\n",
    "    plt.xlabel('Prediction Models')\n",
    "    plt.ylabel('Improvement in Accuracy (%)')\n",
    "    plt.legend(loc='upper right', ncol = 2)\n",
    "#     plt.savefig('exports/predictions/window_methods/mean_improvements_ecoregion.svg')\n",
    "\n",
    "def plot_mean_optimal_errors(opt_error_df):\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    model_labels = ['Linear', 'LSTM', 'GRU']\n",
    "    \n",
    "    width = 0.1\n",
    "    plt.figure(num=None, figsize=(14,10))\n",
    "    \n",
    "    \n",
    "    df_indices = opt_error_df.index.values\n",
    "    xOffset = np.linspace(-0.35,0.35,8)\n",
    "    markerFmts = ['o', '<', 's', '*', 'p', 'x', 'D', '^']\n",
    "    labelArray = ['Fixed, $M^f$ = 5', 'Fixed, $M^f$ = 10', 'Fixed, $M^f$ = 20', 'Fixed, $M^f$ = 40', 'Detected, $\\Delta_{MIN}$ = 5', 'Detected, $\\Delta_{MIN}$ = 10', 'Detected, $\\Delta_{MIN}$ = 20', 'Detected, $\\Delta_{MIN}$ = 30']\n",
    "    \n",
    "    for i in range(len(df_indices)):\n",
    "        plt.plot(x+xOffset[i], opt_error_df.loc[df_indices[i]].values, markerFmts[i], markersize = 10, label= labelArray[i])\n",
    "    \n",
    "    plt.xticks(ticks=x, labels=model_labels)\n",
    "    plt.ylim([0,0.8])\n",
    "    plt.xlabel('Prediction Models')\n",
    "    plt.ylabel('Optimal MAE')\n",
    "    plt.legend(loc='upper right', ncol = 2)\n",
    "#     plt.savefig('exports/predictions/window_methods/mean_opt_mae_ecoregion.svg')\n",
    "\n",
    "def find_improvemnts_over_no_pic(win_error_dict):\n",
    "    metric_index = 1\n",
    "    win_keys = list(win_error_dict.keys())\n",
    "    \n",
    "    win_opt_df = pd.DataFrame()\n",
    "    \n",
    "    linear_opt_error = []\n",
    "    lstm_opt_error = []\n",
    "    gru_opt_error = []\n",
    "\n",
    "    linear_base_error = []\n",
    "    lstm_base_error = []\n",
    "    gru_base_error = []\n",
    "    \n",
    "    for w_key in win_keys:\n",
    "        \n",
    "        test_errors = win_error_dict[w_key]\n",
    "        feature_counts = list(test_errors.keys())\n",
    "        \n",
    "        linear_errors = []\n",
    "        lstm_errors = []\n",
    "        gru_errors = []\n",
    "        \n",
    "        error_df = pd.DataFrame()\n",
    "\n",
    "        \n",
    "        for nPIC in feature_counts:\n",
    "            linear_errors.append(test_errors[nPIC]['Linear'][metric_index])\n",
    "            lstm_errors.append(test_errors[nPIC]['LSTM'][metric_index])\n",
    "            gru_errors.append(test_errors[nPIC]['GRU'][metric_index])\n",
    "        \n",
    "        error_df['nFeat'] = feature_counts\n",
    "        error_df['linear'] = linear_errors\n",
    "        error_df['lstm'] = lstm_errors\n",
    "        error_df['gru'] = gru_errors\n",
    "        error_df = error_df.set_index('nFeat')\n",
    "        \n",
    "        opt_error = error_df.min()\n",
    "        no_pic_error = error_df.loc[0]\n",
    "        \n",
    "\n",
    "        linear_opt_error.append(opt_error.linear)\n",
    "        lstm_opt_error.append(opt_error.lstm)\n",
    "        gru_opt_error.append(opt_error.gru)\n",
    "\n",
    "        linear_base_error.append(no_pic_error.linear)\n",
    "        lstm_base_error.append(no_pic_error.lstm)\n",
    "        gru_base_error.append(no_pic_error.gru)\n",
    "        \n",
    "    win_opt_df['win_key'] = win_keys\n",
    "    win_opt_df = win_opt_df.set_index('win_key')\n",
    "    win_opt_df['linear_base_error'] = linear_base_error\n",
    "    win_opt_df['lstm_base_error'] = lstm_base_error\n",
    "    win_opt_df['gru_base_error'] = gru_base_error\n",
    "    win_opt_df['linear_opt_error'] = linear_opt_error\n",
    "    win_opt_df['lstm_opt_error'] = lstm_opt_error\n",
    "    win_opt_df['gru_opt_error'] = gru_opt_error\n",
    "\n",
    "    win_opt_df[\"linear_improve\"] = -(win_opt_df[\"linear_opt_error\"]-win_opt_df[\"linear_base_error\"])/win_opt_df[\"linear_base_error\"]\n",
    "    win_opt_df[\"lstm_improve\"] = -(win_opt_df[\"lstm_opt_error\"]-win_opt_df[\"lstm_base_error\"])/win_opt_df[\"lstm_base_error\"]\n",
    "    win_opt_df[\"gru_improve\"] = -(win_opt_df[\"gru_opt_error\"]-win_opt_df[\"gru_base_error\"])/win_opt_df[\"gru_base_error\"]\n",
    "    \n",
    "    return win_opt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Select an Ecoregion\n",
    "eco_name = 'Bahia Coastal Forests'\n",
    "\n",
    "# # Find all the locations inside that ecoregion with data\n",
    "# locs_in_region_with_data = get_locations_in_ecoregion(eco_name)\n",
    "# pop_df = get_population_data(locs_in_region_with_data)\n",
    "\n",
    "# # Get incidence data for those locations\n",
    "# inc_count_df = get_data_for_locs(locs_in_region_with_data, False )\n",
    "# inc_count_df = inc_count_df[inc_count_df['date'] < '2019-12-22']\n",
    "\n",
    "# # Save all data read from databases\n",
    "# inc_count_df.to_csv(inc_data_path, index=False)\n",
    "# pop_df.to_csv(pop_data_path, index=True)\n",
    "\n",
    "# Read saved data\n",
    "inc_count_df = pd.read_csv(inc_data_path)\n",
    "pop_df = pd.read_csv(pop_data_path, index_col=0)\n",
    "\n",
    "inc_count_df['date'] = pd.to_datetime(inc_count_df['date'])\n",
    "inc_count_df.iloc[:, 1:] = inc_count_df.iloc[:, 1:].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# Combine the incidence data for the ecoregion\n",
    "ecoregion_df= get_combined_df(pop_df['pop'], inc_count_df)\n",
    "\n",
    "# Drop the locations that have inadequate data ( < 80%)\n",
    "locs_to_drop = get_locs_below_threshold(0.8, inc_count_df)\n",
    "inc_count_df.drop(locs_to_drop, axis=1, inplace=True)\n",
    "locs_to_drop = [int(loc) for loc in locs_to_drop]\n",
    "pop_df.drop(locs_to_drop, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "outbreak_sizes = inc_count_df.sum().sort_values(ascending = False)\n",
    "inc_ratio_df = normalize_case_counts(inc_count_df, pop_df)\n",
    "outbreak_ratios = inc_ratio_df.sum().sort_values(ascending = False)\n",
    "add_case_counts(pop_df, outbreak_sizes, outbreak_ratios)\n",
    "\n",
    "# combine the frames\n",
    "inc_ratio_df['ecoregion'] = ecoregion_df['inc_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get a centroid for the ecoregion\n",
    "# lon = pop_df['lon'].mean()\n",
    "# lat = pop_df['lat'].mean()\n",
    "# print(f'Location coordinates (lat, lon): {lat}, {lon}')\n",
    "\n",
    "# # Get station data\n",
    "# print('Obtaining weather station data ...')\n",
    "# daily_obs_df, dist = get_station_data(lat, lon, search_range=250, urlStation='http://10.7.168.148:3254/stations', urlObs='http://10.7.168.148:3254/observations', fromDate=inc_ratio_df['date'].min().date(), toDate=inc_ratio_df['date'].max().date(), tol=0.1)\n",
    "\n",
    "# # Convert to match the incidence data\n",
    "# Weather_obs_df = convert_weather_to_weekly(inc_ratio_df['date'], daily_obs_df)\n",
    "\n",
    "# # Get reanalysis data\n",
    "# print('Obtaining reanalysis data ...')\n",
    "# Reanalysis_df = get_rean_data(lat, lon, inc_ratio_df['date'])\n",
    "# obs_rean_df = pd.merge(Weather_obs_df, Reanalysis_df, how='outer', on='date')\n",
    "\n",
    "# # Store the data\n",
    "# obs_rean_df.to_pickle(env_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Stored env data\n",
    "obs_rean_df = pd.read_pickle(env_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters: correlation window\n",
    "max_shift = 15\n",
    "comp_win_ext = 4\n",
    "phase_ext = 1\n",
    "min_corr = 0.1\n",
    "\n",
    "sort_options = {}\n",
    "sort_options['corr_weight'] = True\n",
    "sort_options['dist_weight'] = True\n",
    "sort_options['prev_weight'] = True\n",
    "sort_options['prev_type'] = 'relative'\n",
    "\n",
    "\n",
    "# ML Parameters\n",
    "# Prediction window parameters\n",
    "output_steps = 4\n",
    "input_steps = 8\n",
    "\n",
    "# train eval test split: 50 30 20 percents\n",
    "split_frac = [.5, .3, .2]\n",
    "iterations = 150\n",
    "verbosity = 0\n",
    "\n",
    "target_loc = 'ecoregion'\n",
    "\n",
    "feature_counts = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# det win param\n",
    "risk_min = 0.05\n",
    "min_win_lengths = [5, 10, 20, 30]\n",
    "\n",
    "det_win_ranked_pic_dict = {}\n",
    "det_win_error_dict = {}\n",
    "det_win_forecast_win_dict = {}\n",
    "\n",
    "det_win_linear_model_dict = {}\n",
    "det_win_lstm_model_dict = {}\n",
    "det_win_gru_model_dict = {}\n",
    "\n",
    "det_win_optimal_feat_dict = {}\n",
    "\n",
    "\n",
    "for min_win_len in min_win_lengths:\n",
    "    print('For minimum window length of: ' + str(min_win_len))\n",
    "    \n",
    "    print('Computing predictabilities')\n",
    "    pic_predictabilities = compute_var_window_predictability(ic_id = str(target_loc), inc_df = inc_ratio_df, lag_weeks = max_shift, min_win_len=min_win_len, risk_min=risk_min, side_len = comp_win_ext, min_corr = min_corr, extend_phase = phase_ext)\n",
    "    ranked_pics = sort_predictors_for_ecoregion(pic_predictabilities, pop_df, sort_options)\n",
    "    \n",
    "    det_win_ranked_pic_dict[min_win_len] = ranked_pics\n",
    "    \n",
    "    print('Running ML fitting and evaluations')\n",
    "    test_error_dict, forecast_window_dict, linear_model_dict, lstm_model_dict, gru_model_dict = run_ml_predictions(target_loc, obs_rean_df, inc_ratio_df, ranked_pics, feature_counts, split_frac, input_steps, output_steps, iterations, verbosity)\n",
    "    \n",
    "    \n",
    "    det_win_error_dict[min_win_len] = test_error_dict\n",
    "    det_win_forecast_win_dict[min_win_len] = forecast_window_dict\n",
    "\n",
    "    det_win_linear_model_dict[min_win_len] = linear_model_dict\n",
    "    det_win_lstm_model_dict[min_win_len] = lstm_model_dict\n",
    "    det_win_gru_model_dict[min_win_len] = gru_model_dict\n",
    "    \n",
    "    \n",
    "    min_errors = find_best_performers(feature_counts, test_error_dict, 1)\n",
    "    min_errors['loc_id'] = target_loc\n",
    "\n",
    "    optimal_feat_dict = {}\n",
    "    optimal_feat_dict.update(min_errors)\n",
    "    \n",
    "    det_win_optimal_feat_dict[min_win_len] = optimal_feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save simulation results to storage\n",
    "# np.save(ecoregion_data_path+'det_win_ranked_pic_dict.npy', det_win_ranked_pic_dict)\n",
    "# np.save(ecoregion_data_path+'det_win_error_dict.npy', det_win_error_dict)\n",
    "# np.save(ecoregion_data_path+'det_win_optimal_feat_dict.npy', det_win_optimal_feat_dict)\n",
    "\n",
    "# load simulation results from storage\n",
    "det_win_ranked_pic_dict = np.load(ecoregion_data_path+'det_win_ranked_pic_dict.npy', allow_pickle='TRUE').item()\n",
    "det_win_error_dict = np.load(ecoregion_data_path+'det_win_error_dict.npy', allow_pickle='TRUE').item()\n",
    "det_win_optimal_feat_dict = np.load(ecoregion_data_path+'det_win_optimal_feat_dict.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fixed\n",
    "data_splits = [5,10,20,40]\n",
    "\n",
    "fixed_win_ranked_pic_dict = {}\n",
    "fixed_win_error_dict = {}\n",
    "fixed_win_forecast_win_dict = {}\n",
    "\n",
    "fixed_win_linear_model_dict = {}\n",
    "fixed_win_lstm_model_dict = {}\n",
    "fixed_win_gru_model_dict = {}\n",
    "\n",
    "fixed_win_optimal_feat_dict = {}\n",
    "\n",
    "\n",
    "for no_of_windows in data_splits:\n",
    "    print('The number of windows to used: ' + str(no_of_windows))\n",
    "    \n",
    "    print('Computing predictabilities')\n",
    "    pic_predictabilities = compute_predictability(ic_id = 'ecoregion', inc_df = inc_ratio_df, lag_weeks = max_shift, no_of_windows = no_of_windows, side_len = comp_win_ext, min_corr = min_corr, extend_phase = phase_ext)\n",
    "    ranked_pics = sort_predictors_for_ecoregion(pic_predictabilities, pop_df, sort_options)\n",
    "    \n",
    "    fixed_win_ranked_pic_dict[no_of_windows] = ranked_pics\n",
    "    \n",
    "    print('Running ML fitting and evaluations')\n",
    "    test_error_dict, forecast_window_dict, linear_model_dict, lstm_model_dict, gru_model_dict = run_ml_predictions(target_loc, obs_rean_df, inc_ratio_df, ranked_pics, feature_counts, split_frac, input_steps, output_steps, iterations, verbosity)\n",
    "    \n",
    "    \n",
    "    fixed_win_error_dict[no_of_windows] = test_error_dict\n",
    "    fixed_win_forecast_win_dict[no_of_windows] = forecast_window_dict\n",
    "\n",
    "    fixed_win_linear_model_dict[no_of_windows] = linear_model_dict\n",
    "    fixed_win_lstm_model_dict[no_of_windows] = lstm_model_dict\n",
    "    fixed_win_gru_model_dict[no_of_windows] = gru_model_dict\n",
    "    \n",
    "    \n",
    "    min_errors = find_best_performers(feature_counts, test_error_dict, 1)\n",
    "    min_errors['loc_id'] = target_loc\n",
    "\n",
    "    optimal_feat_dict = {}\n",
    "    optimal_feat_dict.update(min_errors)\n",
    "    \n",
    "    fixed_win_optimal_feat_dict[no_of_windows] = optimal_feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save simulation results to storage\n",
    "# np.save(ecoregion_data_path+'fixed_win_ranked_pic_dict.npy', fixed_win_ranked_pic_dict)\n",
    "# np.save(ecoregion_data_path+'fixed_win_error_dict.npy', fixed_win_error_dict)\n",
    "# np.save(ecoregion_data_path+'fixed_win_optimal_feat_dict.npy', fixed_win_optimal_feat_dict)\n",
    "\n",
    "# load simulation results from storage\n",
    "fixed_win_ranked_pic_dict = np.load(ecoregion_data_path+'fixed_win_ranked_pic_dict.npy', allow_pickle='TRUE').item()\n",
    "fixed_win_error_dict = np.load(ecoregion_data_path+'fixed_win_error_dict.npy', allow_pickle='TRUE').item()\n",
    "fixed_win_optimal_feat_dict = np.load(ecoregion_data_path+'fixed_win_optimal_feat_dict.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_opt_error = []\n",
    "lstm_opt_error = []\n",
    "gru_opt_error = []\n",
    "\n",
    "linear_improve = []\n",
    "lstm_improve = []\n",
    "gru_improve = []\n",
    "\n",
    "win_scheme = []\n",
    "\n",
    "win_opt_df = find_improvemnts_over_no_pic(fixed_win_error_dict)\n",
    "win_scheme = ['mf ' + str(x) for x in list(win_opt_df.index)]\n",
    "\n",
    "linear_opt_error += list(win_opt_df.linear_opt_error.values)\n",
    "lstm_opt_error += list(win_opt_df.lstm_opt_error.values)\n",
    "gru_opt_error += list(win_opt_df.gru_opt_error.values)\n",
    "\n",
    "linear_improve += list(win_opt_df.linear_improve.values)\n",
    "lstm_improve += list(win_opt_df.lstm_improve.values)\n",
    "gru_improve += list(win_opt_df.gru_improve.values)\n",
    "\n",
    "win_opt_df = find_improvemnts_over_no_pic(det_win_error_dict)\n",
    "win_scheme = win_scheme + ['dm ' + str(x) for x in list(win_opt_df.index)]\n",
    "\n",
    "linear_opt_error += list(win_opt_df.linear_opt_error.values)\n",
    "lstm_opt_error += list(win_opt_df.lstm_opt_error.values)\n",
    "gru_opt_error += list(win_opt_df.gru_opt_error.values)\n",
    "\n",
    "linear_improve += list(win_opt_df.linear_improve.values)\n",
    "lstm_improve += list(win_opt_df.lstm_improve.values)\n",
    "gru_improve += list(win_opt_df.gru_improve.values)\n",
    "\n",
    "\n",
    "opt_error_df = pd.DataFrame()\n",
    "opt_error_df['schemes'] = win_scheme\n",
    "\n",
    "opt_error_df['linear'] = linear_opt_error\n",
    "opt_error_df['lstm'] = lstm_opt_error\n",
    "opt_error_df['gru'] = gru_opt_error\n",
    "\n",
    "opt_error_df['linear_improve'] = linear_improve\n",
    "opt_error_df['lstm_improve'] = lstm_improve\n",
    "opt_error_df['gru_improve'] = gru_improve\n",
    "\n",
    "opt_error_df.set_index('schemes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_mean_performance_improvements(opt_error_df[['linear_improve', 'lstm_improve', 'gru_improve']])\n",
    "plot_mean_optimal_errors(opt_error_df[['linear', 'lstm', 'gru']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nPIC = 4\n",
    "min_win_len = 30\n",
    "ranked_pics = det_win_ranked_pic_dict[min_win_len]\n",
    "forecast_window = det_win_forecast_win_dict[min_win_len][nPIC]\n",
    "ml_model = det_win_linear_model_dict[min_win_len][nPIC]\n",
    "\n",
    "\n",
    "train_data, val_data, test_data, test_dates, train_mean, train_std, num_features = prepare_data(target_loc, obs_rean_df, inc_ratio_df, ranked_pics[0:nPIC], split_frac, False)\n",
    "\n",
    "time_axis, label_array, prediction_array = forecast_window.combine_shifted_predictions(ml_model)\n",
    "\n",
    "label_array = train_std['cases']*label_array + train_mean['cases']\n",
    "prediction_array = train_std['cases']*prediction_array + train_mean['cases']\n",
    "\n",
    "plt.figure(num=None, figsize=(20,6))\n",
    "plt.scatter(time_axis, label_array, edgecolors='k', c='#2ca02c', s=64, label = 'Actual')\n",
    "plt.scatter(time_axis, prediction_array, marker='X', edgecolors='k', c='#ff7f0e', s=64, label = 'Prediction')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Dengue cases per 100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import folium.plugins as foplug\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "nSelect = 40\n",
    "picSelect = np.arange(0,nSelect)\n",
    "\n",
    "# Select a bunch of points (based on pic ranks)\n",
    "heatData = []\n",
    "for idx in picSelect:\n",
    "    loc_id = ranked_pics[idx]\n",
    "    pic_data = pop_df.loc[int(loc_id)]\n",
    "    nPIC = ranked_pics.shape[0]\n",
    "    weight = (nPIC-idx)/nPIC\n",
    "    \n",
    "    heatData.append([pic_data.lat, pic_data.lon, weight])\n",
    "\n",
    "\n",
    "heatTimeData = [heatData]*len(prediction_array)\n",
    "heatTimeData = np.array(heatTimeData)\n",
    "\n",
    "for idx, val in enumerate(prediction_array):\n",
    "    heatTimeData[idx,:,2] = heatTimeData[idx,:,2]*val\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0.3,.8))\n",
    "heatTimeData[:,:,2] = scaler.fit_transform(heatTimeData[:,:,2])\n",
    "\n",
    "eco_coords = [pop_df['lat'].mean(), pop_df['lon'].mean()]\n",
    "map_handle = folium.Map(location = eco_coords, zoom_start = 9, tiles='cartodbpositron')\n",
    "\n",
    "# Ecoregion database not accessible at this moment, hence the boundary can't be drawn\n",
    "# region = get_ecoregion_boundaries(eco_name)\n",
    "# styleDict = {'fillColor': 'blue', 'color': 'red', 'weight': 2, 'opacity':0.3, 'fillOpacity':0 }\n",
    "# folium.GeoJson(\n",
    "#     region,\n",
    "#     name='geojson',\n",
    "#     style_function = lambda x: styleDict\n",
    "# ).add_to(map_handle)\n",
    "\n",
    "# hm = foplug.HeatMap(heatData)\n",
    "# hm.add_to(map_handle)\n",
    "\n",
    "taxis = np.array(pd.to_datetime(time_axis).strftime('%Y-%m-%d')).tolist()\n",
    "hm = foplug.HeatMapWithTime(data = heatTimeData.tolist(), index = taxis, radius = 25)\n",
    "hm.add_to(map_handle)\n",
    "\n",
    "f = folium.Figure(width=800, height=600)\n",
    "f.add_child(map_handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
